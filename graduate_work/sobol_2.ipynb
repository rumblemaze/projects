{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5debeee-6c6c-46ba-b5f2-5507f5db7d20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5263b3-1cd5-4fd7-ab04-a82248b7f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import os\n",
    "import collections\n",
    "import seaborn as sns \n",
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol\n",
    "\n",
    "from sklearn import model_selection, ensemble, utils\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b370c4ba-4a8b-4193-a3f4-4af6de8b2ac9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Генерация выборки для предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7ed4657-157a-4cd4-b8e6-c357cb093680",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = {\n",
    "    'num_vars': 28,\n",
    "    'names': list(data_X.columns),\n",
    "    'bounds': [[data_X['boundary_condition_NZ'].min(), data_X['boundary_condition_NZ'].max()],\n",
    "               [data_X['boundary_condition_X'].min(), data_X['boundary_condition_X'].max()],\n",
    "               [data_X['boundary_condition_Y'].min(), data_X['boundary_condition_Y'].max()],\n",
    "               [data_X['boundary_condition_NX'].min(), data_X['boundary_condition_NX'].max()],\n",
    "               [data_X['boundary_condition_NY'].min(), data_X['boundary_condition_NY'].max()],\n",
    "               [data_X['boundary_condition_Z'].min(), data_X['boundary_condition_Z'].max()],\n",
    "               [data_X['load_time'].min(), data_X['load_time'].max()],\n",
    "               [data_X['rock_young_constant'].min(), data_X['rock_young_constant'].max()],\n",
    "               [data_X['rock_alpha_constant'].min(), data_X['rock_alpha_constant'].max()],\n",
    "               [data_X['concrete_init_temp'].min(), data_X['concrete_init_temp'].max()],\n",
    "               [data_X['concrete_cheat'].min(), data_X['concrete_cheat'].max()],\n",
    "               [data_X['concrete_dt'].min(), data_X['concrete_dt'].max()],\n",
    "               [data_X['concrete_norm_coeff'].min(), data_X['concrete_norm_coeff'].max()],\n",
    "               [data_X['concrete_young_constant'].min(), data_X['concrete_young_constant'].max()],\n",
    "               [data_X['concrete_alpha_constant'].min(), data_X['concrete_alpha_constant'].max()],\n",
    "               [data_X['concrete_strength_time'].min(), data_X['concrete_strength_time'].max()],\n",
    "               [data_X['steel_init_temp'].min(), data_X['steel_init_temp'].max()],\n",
    "               [data_X['bentonite_init_temp'].min(), data_X['bentonite_init_temp'].max()],\n",
    "               [data_X['bentonite_cheat'].min(), data_X['bentonite_cheat'].max()],\n",
    "               [data_X['bentonite_dt'].min(), data_X['bentonite_dt'].max()],\n",
    "               [data_X['bentonite_young_constant'].min(), data_X['bentonite_young_constant'].max()],\n",
    "               [data_X['bentonite_alpha_constant'].min(), data_X['bentonite_alpha_constant'].max()],\n",
    "               [data_X['rw_init_temp'].min(), data_X['rw_init_temp'].max()],\n",
    "               [data_X['rw_cheat'].min(), data_X['rw_cheat'].max()],\n",
    "               [data_X['rw_dt'].min(), data_X['rw_dt'].max()],\n",
    "               [data_X['rw_young_constant'].min(), data_X['rw_young_constant'].max()],\n",
    "               [data_X['rw_alpha_constant'].min(), data_X['rw_alpha_constant'].max()],\n",
    "               [data_X['rw_norm_coeff'].min(), data_X['rw_norm_coeff'].max()]]\n",
    "}\n",
    "\n",
    "\n",
    "param_values = saltelli.sample(problem, 128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcec724-2581-4fd0-ac42-e18dadf1f3c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08e68479-cf74-499c-93c7-d5d0d7dd4424",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ensemble.RandomForestRegressor(n_estimators = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0fb4360-3324-4f89-8a27-a0b28ace402c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_X = pd.read_csv('sample.csv')\n",
    "data_X.drop('calc_id', 1, inplace = True)\n",
    "data_X = data_X.iloc[:-1576]\n",
    "data_X.load_time = data_X.load_time.round(decimals = 0)\n",
    "\n",
    "points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "for k in range(1,14,1):\n",
    "    observe_point = k\n",
    "    points.remove(observe_point)\n",
    "    \n",
    "    data_temp = pd.read_csv('temp.csv')\n",
    "    data_temp.drop('calc_id', 1, inplace = True)\n",
    "    \n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "        for i in points:\n",
    "            data_temp.drop(columns = 'temp_point'+str(i)+'_'+str(j)+'days', inplace = True)\n",
    "    data_temp = data_temp.iloc[:-1576]\n",
    "\n",
    "\n",
    "\n",
    "    sobol_indices = pd.DataFrame(index = list(data_X.columns))\n",
    "    feature_importances = pd.DataFrame(index = list(data_X.columns))\n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "\n",
    "        rf.fit(data_X, data_temp['temp_point' + str(observe_point)+ '_' + str(j) + 'days'])\n",
    "        predicted = rf.predict(param_values)\n",
    "        feature_importances.insert(len(feature_importances.columns),'FI_'+str(observe_point)+'_'+str(j)+'days',  rf.feature_importances_)\n",
    "        \n",
    "        Si = sobol.analyze(problem, predicted)\n",
    "        sobol_indices.insert(len(sobol_indices.columns), 'ST_'+str(observe_point)+'_'+str(j)+'days' ,Si['ST'])\n",
    "    feature_importances.to_csv('feature_importances_temp_point' +str(k)+'.csv', float_format='%.6f')\n",
    "    sobol_indices.to_csv('sobol_indices_temp_point' +str(k)+'.csv', float_format='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0dbe522-c376-4139-ad8c-4398e4d5225b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_X = pd.read_csv('sample.csv')\n",
    "data_X.drop('calc_id', 1, inplace = True)\n",
    "data_X = data_X.iloc[:-1576]\n",
    "data_X.load_time = data_X.load_time.round(decimals = 0)\n",
    "\n",
    "\n",
    "points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "for k in range(1,14,1):\n",
    "    observe_point = k\n",
    "    points.remove(observe_point)\n",
    "    \n",
    "    data_stress = pd.read_csv('stress.csv')\n",
    "    data_stress.drop('calc_id', 1, inplace = True)\n",
    "    \n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "        for i in points:\n",
    "            data_stress.drop(columns = 'stress_point'+str(i)+'_'+str(j)+'days', inplace = True)\n",
    "    data_stress = data_stress.iloc[:-1576]\n",
    "\n",
    "\n",
    "\n",
    "    sobol_indices = pd.DataFrame(index = list(data_X.columns))\n",
    "    feature_importances = pd.DataFrame(index = list(data_X.columns))\n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "\n",
    "        rf.fit(data_X, data_stress['stress_point' + str(observe_point)+ '_' + str(j) + 'days'])\n",
    "        predicted = rf.predict(param_values)\n",
    "        Si = sobol.analyze(problem, predicted)\n",
    "        feature_importances.insert(len(feature_importances.columns),'FI_'+str(observe_point)+'_'+str(j)+'days',  rf.feature_importances_)\n",
    "        sobol_indices.insert(len(sobol_indices.columns), 'ST_'+str(observe_point)+'_'+str(j)+'days' ,Si['ST'])\n",
    "\n",
    "    sobol_indices.to_csv('sobol_indices_stress_point' +str(k)+'.csv', float_format='%.7f')\n",
    "    feature_importances.to_csv('feature_importances_stress_point' +str(k)+'.csv', float_format='%.6f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6ecf9-4876-4c8c-99aa-8947982eb0f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2700562-2d36-4a7c-87d7-357ca03b91cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB = xgboost.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "316de2f8-79d0-4944-9f32-2f60ee2092ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_X = pd.read_csv('sample.csv')\n",
    "data_X.drop('calc_id', 1, inplace = True)\n",
    "data_X = data_X.iloc[:-1576]\n",
    "data_X.load_time = data_X.load_time.round(decimals = 0)\n",
    "\n",
    "points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "for k in range(1,14,1):\n",
    "    observe_point = k\n",
    "    points.remove(observe_point)\n",
    "    \n",
    "    data_temp = pd.read_csv('temp.csv')\n",
    "    data_temp.drop('calc_id', 1, inplace = True)\n",
    "    \n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "        for i in points:\n",
    "            data_temp.drop(columns = 'temp_point'+str(i)+'_'+str(j)+'days', inplace = True)\n",
    "    data_temp = data_temp.iloc[:-1576]\n",
    "\n",
    "\n",
    "\n",
    "    sobol_indices = pd.DataFrame(index = list(data_X.columns))\n",
    "    feature_importances = pd.DataFrame(index = list(data_X.columns))\n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "\n",
    "        XGB.fit(data_X, data_temp['temp_point' + str(observe_point)+ '_' + str(j) + 'days'])\n",
    "        predicted = XGB.predict(param_values)\n",
    "        feature_importances.insert(len(feature_importances.columns),'FI_'+str(observe_point)+'_'+str(j)+'days',  XGB.feature_importances_)\n",
    "        \n",
    "        Si = sobol.analyze(problem, predicted)\n",
    "        sobol_indices.insert(len(sobol_indices.columns), 'ST_'+str(observe_point)+'_'+str(j)+'days' ,Si['ST'])\n",
    "    feature_importances.to_csv('feature_importances_temp_point' +str(k)+'.csv', float_format='%.6f')\n",
    "    sobol_indices.to_csv('sobol_indices_temp_point' +str(k)+'.csv', float_format='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21746c74-844e-4c35-9514-19f39cf2226f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Install\\Anaconda3\\lib\\site-packages\\SALib\\analyze\\sobol.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.mean(B * (AB - A), axis=0) / np.var(np.r_[A, B], axis=0)\n",
      "D:\\Install\\Anaconda3\\lib\\site-packages\\SALib\\analyze\\sobol.py:161: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.mean(B * (AB - A), axis=0) / np.var(np.r_[A, B], axis=0)\n",
      "D:\\Install\\Anaconda3\\lib\\site-packages\\SALib\\analyze\\sobol.py:169: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 0.5 * np.mean((A - AB) ** 2, axis=0) / np.var(np.r_[A, B], axis=0)\n",
      "D:\\Install\\Anaconda3\\lib\\site-packages\\SALib\\analyze\\sobol.py:169: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return 0.5 * np.mean((A - AB) ** 2, axis=0) / np.var(np.r_[A, B], axis=0)\n",
      "D:\\Install\\Anaconda3\\lib\\site-packages\\SALib\\analyze\\sobol.py:174: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  Vjk = np.mean(BAj * ABk - A * B, axis=0) / np.var(np.r_[A, B], axis=0)\n",
      "D:\\Install\\Anaconda3\\lib\\site-packages\\SALib\\analyze\\sobol.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Vjk = np.mean(BAj * ABk - A * B, axis=0) / np.var(np.r_[A, B], axis=0)\n",
      "D:\\Install\\Anaconda3\\lib\\site-packages\\SALib\\analyze\\sobol.py:96: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Y = (Y - Y.mean()) / Y.std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_X = pd.read_csv('sample.csv')\n",
    "data_X.drop('calc_id', 1, inplace = True)\n",
    "data_X = data_X.iloc[:-1576]\n",
    "data_X.load_time = data_X.load_time.round(decimals = 0)\n",
    "\n",
    "points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "for k in range(1,14,1):\n",
    "    observe_point = k\n",
    "    points.remove(observe_point)\n",
    "    \n",
    "    data_stress = pd.read_csv('stress.csv')\n",
    "    data_stress.drop('calc_id', 1, inplace = True)\n",
    "    \n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "        for i in points:\n",
    "            data_stress.drop(columns = 'stress_point'+str(i)+'_'+str(j)+'days', inplace = True)\n",
    "    data_stress = data_stress.iloc[:-1576]\n",
    "\n",
    "\n",
    "\n",
    "    sobol_indices = pd.DataFrame(index = list(data_X.columns))\n",
    "    feature_importances = pd.DataFrame(index = list(data_X.columns))\n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "\n",
    "        XGB.fit(data_X, data_stress['stress_point' + str(observe_point)+ '_' + str(j) + 'days'])\n",
    "        predicted = XGB.predict(param_values)\n",
    "        Si = sobol.analyze(problem, predicted)\n",
    "        feature_importances.insert(len(feature_importances.columns),'FI_'+str(observe_point)+'_'+str(j)+'days',  XGB.feature_importances_)\n",
    "        sobol_indices.insert(len(sobol_indices.columns), 'ST_'+str(observe_point)+'_'+str(j)+'days' ,Si['ST'])\n",
    "\n",
    "    sobol_indices.to_csv('sobol_indices_stress_point' +str(k)+'.csv', float_format='%.7f')\n",
    "    feature_importances.to_csv('feature_importances_stress_point' +str(k)+'.csv', float_format='%.6f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eef4995-be9d-469b-a6d7-315f3fcb905e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f85cf5-bb3c-4547-bab6-5f1f8d12788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'max_depth': [50, 75, 100, 125],\n",
    "    'n_estimators': [75, 100, 125, 150]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "    data_temp = pd.read_csv('temp.csv')\n",
    "    data_temp.drop('calc_id', 1, inplace = True)\n",
    "    data_temp = data_temp.iloc[:-1576]\n",
    "\n",
    "grid_search.fit(data_X, data_temp['temp_point' + str(5)+ '_' + str(40) + 'days'])\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b80276-d1f2-47a2-a00d-22116b9064ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Пример работы для диплома (useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77caf63d-9a61-44fe-9783-85b2e9793fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ensemble.RandomForestRegressor(max_depth = 125,\n",
    " max_features = None,\n",
    " min_samples_leaf = 1,\n",
    " n_estimators = 150)\n",
    "\n",
    "S_indices = pd.DataFrame(index = list(data_X.columns))\n",
    "\n",
    "rf.fit(data_X, data_temp['temp_point5_40days'])\n",
    "Si = sobol.analyze(problem, rf.predict(param_values))\n",
    "S_indices.insert(len(S_indices.columns), 'ST_5_40_days' ,Si['ST'])\n",
    "\n",
    "S_indices.sort_values(by = ['ST_5_40_days'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f558e-c777-49fa-970e-5592e9ffed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp['temp_point5_40days'][88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f3c14d-c412-42d8-a73d-953331a194d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9956628086647825"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection.cross_val_score(rf, data_X, data_temp['temp_point5_5days'],).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "702b5e3a-8184-49bb-8967-86854eac7494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9952007809796971"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection.cross_val_score(XGB, data_X, data_temp['temp_point7_5days'], ).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d86d1f0-6653-4517-ae16-17dfd3323f78",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09184c87-56fe-40a1-8919-a098e75d778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlargest = np.ndarray(0)\n",
    "points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "for k in range(1,14,1):\n",
    "    observe_point = k\n",
    "    points.remove(observe_point) \n",
    "    sobol_indices = pd.read_csv('C:\\\\Users\\\\defuz\\\\JupyterLab\\\\data diplom\\\\load_time_rounded\\\\RandomForest_results\\\\sobol_indicess_temp_point'+str(observe_point)+'.csv',\n",
    "                           index_col = 0)\n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "        nlargest = np.append(nlargest, sobol_indices.nlargest(5, 'ST_' + str(observe_point) + '_' + str(j) + 'days')['ST_' + str(observe_point) + '_' + str(j) + 'days'].index.to_numpy())\n",
    "\n",
    "SI_temp_counter = pd.DataFrame.from_dict(dict(collections.Counter(nlargest)), orient='index')\n",
    "SI_temp_counter.sort_values(by = 0, ascending = False).to_csv('RF_SI_temp.csv')\n",
    "\n",
    "nlargest = np.ndarray(0)\n",
    "points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "for k in range(1,14,1):\n",
    "    observe_point = k\n",
    "    points.remove(observe_point) \n",
    "    sobol_indices = pd.read_csv('C:\\\\Users\\\\defuz\\\\JupyterLab\\\\data diplom\\\\load_time_rounded\\\\RandomForest_results\\\\sobol_indices_stress_point'+str(observe_point)+'.csv',\n",
    "                           index_col = 0)\n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "        nlargest = np.append(nlargest, sobol_indices.nlargest(5, 'ST_' + str(observe_point) + '_' + str(j) + 'days')['ST_' + str(observe_point) + '_' + str(j) + 'days'].index.to_numpy())\n",
    "\n",
    "SI_stress_counter = pd.DataFrame.from_dict(dict(collections.Counter(nlargest)), orient='index')\n",
    "SI_stress_counter.sort_values(by = 0, ascending = False).to_csv('RF_SI_stress.csv')\n",
    "\n",
    "nlargest = np.ndarray(0)\n",
    "points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "for k in range(1,14,1):\n",
    "    observe_point = k\n",
    "    points.remove(observe_point) \n",
    "    feature_importances = pd.read_csv('C:\\\\Users\\\\defuz\\\\JupyterLab\\\\data diplom\\\\load_time_rounded\\\\RandomForest_results\\\\feature_importances_temp_point'+str(observe_point)+'.csv',\n",
    "                           index_col = 0)\n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "        nlargest = np.append(nlargest, feature_importances.nlargest(5, 'FI_' + str(observe_point) + '_' + str(j) + 'days')['FI_' + str(observe_point) + '_' + str(j) + 'days'].index.to_numpy())\n",
    "\n",
    "FI_temp_counter = pd.DataFrame.from_dict(dict(collections.Counter(nlargest)), orient='index')\n",
    "FI_temp_counter.sort_values(by = 0, ascending = False).to_csv('RF_FI_temp.csv')\n",
    "\n",
    "nlargest = np.ndarray(0)\n",
    "points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "for k in range(1,14,1):\n",
    "    observe_point = k\n",
    "    points.remove(observe_point) \n",
    "    feature_importances = pd.read_csv('C:\\\\Users\\\\defuz\\\\JupyterLab\\\\data diplom\\\\load_time_rounded\\\\RandomForest_results\\\\feature_importances_stress_point'+str(observe_point)+'.csv',\n",
    "                           index_col = 0)\n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "        nlargest = np.append(nlargest, feature_importances.nlargest(5, 'FI_' + str(observe_point) + '_' + str(j) + 'days')['FI_' + str(observe_point) + '_' + str(j) + 'days'].index.to_numpy())\n",
    "\n",
    "FI_stress_counter = pd.DataFrame.from_dict(dict(collections.Counter(nlargest)), orient='index')\n",
    "FI_stress_counter.sort_values(by = 0, ascending = False).to_csv('RF_FI_stress.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0f948c2-3f52-4bf9-abfb-0b2809008bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlargest = np.ndarray(0)\n",
    "points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "for k in range(1,14,1):\n",
    "    observe_point = k\n",
    "    points.remove(observe_point) \n",
    "    sobol_indices = pd.read_csv('C:\\\\Users\\\\defuz\\\\JupyterLab\\\\data diplom\\\\load_time_rounded\\\\XGBoost_results\\\\sobol_indicess_temp_point'+str(observe_point)+'.csv',\n",
    "                           index_col = 0)\n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "        nlargest = np.append(nlargest, sobol_indices.nlargest(5, 'ST_' + str(observe_point) + '_' + str(j) + 'days')['ST_' + str(observe_point) + '_' + str(j) + 'days'].index.to_numpy())\n",
    "\n",
    "SI_temp_counter = pd.DataFrame.from_dict(dict(collections.Counter(nlargest)), orient='index')\n",
    "SI_temp_counter.sort_values(by = 0, ascending = False).to_csv('XGB_SI_temp.csv')\n",
    "\n",
    "nlargest = np.ndarray(0)\n",
    "points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "for k in range(1,14,1):\n",
    "    observe_point = k\n",
    "    points.remove(observe_point) \n",
    "    sobol_indices = pd.read_csv('C:\\\\Users\\\\defuz\\\\JupyterLab\\\\data diplom\\\\load_time_rounded\\\\XGBoost_results\\\\sobol_indices_stress_point'+str(observe_point)+'.csv',\n",
    "                           index_col = 0)\n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "        nlargest = np.append(nlargest, sobol_indices.nlargest(5, 'ST_' + str(observe_point) + '_' + str(j) + 'days')['ST_' + str(observe_point) + '_' + str(j) + 'days'].index.to_numpy())\n",
    "\n",
    "SI_stress_counter = pd.DataFrame.from_dict(dict(collections.Counter(nlargest)), orient='index')\n",
    "SI_stress_counter.sort_values(by = 0, ascending = False).to_csv('XGB_SI_stress.csv')\n",
    "\n",
    "nlargest = np.ndarray(0)\n",
    "points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "for k in range(1,14,1):\n",
    "    observe_point = k\n",
    "    points.remove(observe_point) \n",
    "    feature_importances = pd.read_csv('C:\\\\Users\\\\defuz\\\\JupyterLab\\\\data diplom\\\\load_time_rounded\\\\XGBoost_results\\\\feature_importances_temp_point'+str(observe_point)+'.csv',\n",
    "                           index_col = 0)\n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "        nlargest = np.append(nlargest, feature_importances.nlargest(5, 'FI_' + str(observe_point) + '_' + str(j) + 'days')['FI_' + str(observe_point) + '_' + str(j) + 'days'].index.to_numpy())\n",
    "\n",
    "FI_temp_counter = pd.DataFrame.from_dict(dict(collections.Counter(nlargest)), orient='index')\n",
    "FI_temp_counter.sort_values(by = 0, ascending = False).to_csv('XGB_FI_temp.csv')\n",
    "\n",
    "nlargest = np.ndarray(0)\n",
    "points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "for k in range(1,14,1):\n",
    "    observe_point = k\n",
    "    points.remove(observe_point) \n",
    "    feature_importances = pd.read_csv('C:\\\\Users\\\\defuz\\\\JupyterLab\\\\data diplom\\\\load_time_rounded\\\\XGBoost_results\\\\feature_importances_stress_point'+str(observe_point)+'.csv',\n",
    "                           index_col = 0)\n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "        nlargest = np.append(nlargest, feature_importances.nlargest(5, 'FI_' + str(observe_point) + '_' + str(j) + 'days')['FI_' + str(observe_point) + '_' + str(j) + 'days'].index.to_numpy())\n",
    "\n",
    "FI_stress_counter = pd.DataFrame.from_dict(dict(collections.Counter(nlargest)), orient='index')\n",
    "FI_stress_counter.sort_values(by = 0, ascending = False).to_csv('XGB_FI_stress.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99b1f62-e03b-4e5e-8d13-5ea2f8fee899",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### XGBoost samples variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d2b966f8-f1f0-4b8e-9fe9-90069d36df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB = xgboost.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95c5376d-0b4b-46b9-841b-14fa4d11101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for n in [1,2,3,4,5]:\n",
    "    \n",
    "    param_values = saltelli.sample(problem, pow(2,n))\n",
    "\n",
    "    data_X = pd.read_csv('sample.csv')\n",
    "    data_X.drop('calc_id', 1, inplace = True)\n",
    "    data_X = data_X.iloc[:-(9000-pd.DataFrame(param_values).shape[0])]\n",
    "    data_X.load_time = data_X.load_time.round(decimals = 0)\n",
    "\n",
    "    points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "    for k in range(1,14,1):\n",
    "        \n",
    "        observe_point = k\n",
    "        points.remove(observe_point)\n",
    "\n",
    "        data_temp = pd.read_csv('temp.csv')\n",
    "        data_temp.drop('calc_id', 1, inplace = True)\n",
    "        data_temp = data_temp.iloc[:-(9000-pd.DataFrame(param_values).shape[0])]\n",
    "        data = data_X.join(data_temp)\n",
    "        \n",
    "        for j in [5,10,20,30,40,50,150,1000]:\n",
    "            for i in points:\n",
    "                data.drop(columns = 'temp_point'+str(i)+'_'+str(j)+'days', inplace = True)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        sobol_indices = pd.DataFrame(index = list(data_X.columns))\n",
    "        feature_importances = pd.DataFrame(index = list(data_X.columns))\n",
    "        CVS_df = pd.DataFrame(index = ['cross_val_score'])\n",
    "        \n",
    "        for j in [5,10,20,30,40,50,150,1000]:\n",
    "\n",
    "            CVS = model_selection.cross_val_score(XGB, data_X, data['temp_point' + str(observe_point)+ '_' + str(j) + 'days']).mean()\n",
    "            CVS_df.insert(len(CVS_df.columns), 'CVS_'+str(observe_point)+'_'+str(j)+'days', CVS)\n",
    "            XGB.fit(data_X, data['temp_point' + str(observe_point)+ '_' + str(j) + 'days'])\n",
    "            predicted = XGB.predict(param_values)\n",
    "            feature_importances.insert(len(feature_importances.columns),'FI_'+str(observe_point)+'_'+str(j)+'days',  XGB.feature_importances_)\n",
    "\n",
    "            Si = sobol.analyze(problem, predicted)\n",
    "            sobol_indices.insert(len(sobol_indices.columns), 'ST_'+str(observe_point)+'_'+str(j)+'days' ,Si['ST'])\n",
    "        feature_importances.to_csv(str(n)+'_'+'feature_importances_temp_point' +str(k)+'.csv', float_format='%.6f')\n",
    "        sobol_indices.to_csv(str(n)+'_'+'sobol_indicess_temp_point' +str(k)+'.csv', float_format='%.6f')\n",
    "        CVS_df.to_csv(str(n)+'_'+'cross_validation_temp_point' +str(k)+'.csv', float_format='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53066067-a3e3-4084-89ec-002b86328132",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "for n in [1,2,3,4,5]:\n",
    "    \n",
    "    param_values = saltelli.sample(problem, pow(2,n))\n",
    "\n",
    "    data_X = pd.read_csv('sample.csv')\n",
    "    data_X.drop('calc_id', 1, inplace = True)\n",
    "    data_X = data_X.iloc[:-(9000-pd.DataFrame(param_values).shape[0])]\n",
    "    data_X.load_time = data_X.load_time.round(decimals = 0)\n",
    "\n",
    "    points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "    for k in range(1,14,1):\n",
    "        observe_point = k\n",
    "        points.remove(observe_point)\n",
    "\n",
    "        data_stress = pd.read_csv('stress.csv')\n",
    "        data_stress.drop('calc_id', 1, inplace = True)\n",
    "\n",
    "        for j in [5,10,20,30,40,50,150,1000]:\n",
    "            for i in points:\n",
    "                data_stress.drop(columns = 'stress_point'+str(i)+'_'+str(j)+'days', inplace = True)\n",
    "        data_stress = data_stress.iloc[:-(9000-pd.DataFrame(param_values).shape[0])]\n",
    "\n",
    "\n",
    "\n",
    "        sobol_indices = pd.DataFrame(index = list(data_X.columns))\n",
    "        feature_importances = pd.DataFrame(index = list(data_X.columns))\n",
    "        CVS_df = pd.DataFrame(index = ['cross_val_score'])\n",
    "        for j in [5,10,20,30,40,50,150,1000]:\n",
    "\n",
    "            CVS = model_selection.cross_val_score(XGB, data_X, data_stress['stress_point' + str(observe_point)+ '_' + str(j) + 'days']).mean()\n",
    "            CVS_df.insert(len(CVS_df.columns), 'CVS_'+str(observe_point)+'_'+str(j)+'days', CVS)\n",
    "            XGB.fit(data_X, data_stress['stress_point' + str(observe_point)+ '_' + str(j) + 'days'])\n",
    "            predicted = XGB.predict(param_values)\n",
    "            Si = sobol.analyze(problem, predicted)\n",
    "            feature_importances.insert(len(feature_importances.columns),'FI_'+str(observe_point)+'_'+str(j)+'days',  XGB.feature_importances_)\n",
    "            sobol_indices.insert(len(sobol_indices.columns), 'ST_'+str(observe_point)+'_'+str(j)+'days' ,Si['ST'])\n",
    "\n",
    "        sobol_indices.to_csv(str(n)+'_'+'sobol_indices_stress_point' +str(k)+'.csv', float_format='%.7f')\n",
    "        feature_importances.to_csv(str(n)+'_'+'feature_importances_stress_point' +str(k)+'.csv', float_format='%.6f')\n",
    "        CVS_df.to_csv(str(n)+'_'+'cross_validation_stress_point' +str(k)+'.csv', float_format='%.6f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4831ed6e-ab46-440d-b3ef-a95f98c9a094",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Counter variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d373675-f325-4bb5-8d43-d7c45b00d017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e9d485fa-a127-489b-bf3a-98555e5b1be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [1,2,3,4,5]:\n",
    "    nlargest = np.ndarray(0)\n",
    "    points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "    for k in range(1,14,1):\n",
    "        observe_point = k\n",
    "        points.remove(observe_point) \n",
    "        sobol_indices = pd.read_csv('C:\\\\Users\\\\defuz\\\\JupyterLab\\\\data diplom\\\\XGB_variety\\\\'+str(n)+'_'+'sobol_indicess_temp_point'+str(observe_point)+'.csv',\n",
    "                               index_col = 0)\n",
    "        for j in [20,30,40,50,150,1000]:\n",
    "            nlargest = np.append(nlargest, sobol_indices.nlargest(5, 'ST_' + str(observe_point) + '_' + str(j) + 'days')['ST_' + str(observe_point) + '_' + str(j) + 'days'].index.to_numpy())\n",
    "\n",
    "    SI_temp_counter = pd.DataFrame.from_dict(dict(collections.Counter(nlargest)), orient='index')\n",
    "    SI_temp_counter.sort_values(by = 0, ascending = False).to_csv(str(n)+'_'+'XGB_SI_temp.csv')\n",
    "\n",
    "    nlargest = np.ndarray(0)\n",
    "    points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "    for k in range(1,14,1):\n",
    "        observe_point = k\n",
    "        points.remove(observe_point) \n",
    "        sobol_indices = pd.read_csv('C:\\\\Users\\\\defuz\\\\JupyterLab\\\\data diplom\\\\XGB_variety\\\\'+str(n)+'_'+'sobol_indices_stress_point'+str(observe_point)+'.csv',\n",
    "                               index_col = 0)\n",
    "        for j in [20,30,40,50,150,1000]:\n",
    "            nlargest = np.append(nlargest, sobol_indices.nlargest(5, 'ST_' + str(observe_point) + '_' + str(j) + 'days')['ST_' + str(observe_point) + '_' + str(j) + 'days'].index.to_numpy())\n",
    "\n",
    "    SI_stress_counter = pd.DataFrame.from_dict(dict(collections.Counter(nlargest)), orient='index')\n",
    "    SI_stress_counter.sort_values(by = 0, ascending = False).to_csv(str(n)+'_'+'XGB_SI_stress.csv')\n",
    "\n",
    "    nlargest = np.ndarray(0)\n",
    "    points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "    for k in range(1,14,1):\n",
    "        observe_point = k\n",
    "        points.remove(observe_point) \n",
    "        feature_importances = pd.read_csv('C:\\\\Users\\\\defuz\\\\JupyterLab\\\\data diplom\\\\XGB_variety\\\\'+str(n)+'_'+'feature_importances_temp_point'+str(observe_point)+'.csv',\n",
    "                               index_col = 0)\n",
    "        for j in [20,30,40,50,150,1000]:\n",
    "            nlargest = np.append(nlargest, feature_importances.nlargest(5, 'FI_' + str(observe_point) + '_' + str(j) + 'days')['FI_' + str(observe_point) + '_' + str(j) + 'days'].index.to_numpy())\n",
    "\n",
    "    FI_temp_counter = pd.DataFrame.from_dict(dict(collections.Counter(nlargest)), orient='index')\n",
    "    FI_temp_counter.sort_values(by = 0, ascending = False).to_csv(str(n)+'_'+'XGB_FI_temp.csv')\n",
    "\n",
    "    nlargest = np.ndarray(0)\n",
    "    points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "    for k in range(1,14,1):\n",
    "        observe_point = k\n",
    "        points.remove(observe_point) \n",
    "        feature_importances = pd.read_csv('C:\\\\Users\\\\defuz\\\\JupyterLab\\\\data diplom\\\\XGB_variety\\\\'+str(n)+'_'+'feature_importances_stress_point'+str(observe_point)+'.csv',\n",
    "                               index_col = 0)\n",
    "        for j in [20,30,40,50,150,1000]:\n",
    "            nlargest = np.append(nlargest, feature_importances.nlargest(5, 'FI_' + str(observe_point) + '_' + str(j) + 'days')['FI_' + str(observe_point) + '_' + str(j) + 'days'].index.to_numpy())\n",
    "\n",
    "    FI_stress_counter = pd.DataFrame.from_dict(dict(collections.Counter(nlargest)), orient='index')\n",
    "    FI_stress_counter.sort_values(by = 0, ascending = False).to_csv(str(n)+'_'+'XGB_FI_stress.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7d2964-a848-4eb2-89eb-b0a94f5aba38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8810188d-ed33-42be-acc8-ce54914b307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlargest = np.ndarray(0)\n",
    "\n",
    "sobol_indices = pd.read_csv('C:\\\\Users\\\\defuz\\\\JupyterLab\\\\data diplom\\\\load_time_rounded\\\\XGBoost_results\\\\sobol_indicess_temp_point'+str(5)+'.csv',\n",
    "                           index_col = 0)\n",
    "\n",
    "nlargest = np.append(nlargest, sobol_indices.nlargest(5, 'ST_' + str(5) + '_' + str(40) + 'days')['ST_' + str(5) + '_' + str(40) + 'days'].index.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "95405ec7-3b45-43b7-81a9-2ddfd40dd0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.027307e+00, 7.570720e-01, 3.176210e-01, 1.114820e-01,\n",
       "       4.074400e-02, 3.743800e-02, 8.294000e-03, 1.000000e-05])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sobol_indices.loc['load_time',:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80479860-60e8-40cd-be69-23416dce660d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
