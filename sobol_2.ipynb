{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb86a86f-9547-4ac5-bf39-17817b50d778",
   "metadata": {},
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ec5263b3-1cd5-4fd7-ab04-a82248b7f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol\n",
    "\n",
    "from sklearn import model_selection, ensemble, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b924eda-c83b-4885-8d7b-5ef83b89a414",
   "metadata": {},
   "source": [
    "### Считывание входных параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "78a93e64-e461-4250-ac68-c467daa0a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = pd.read_csv('sample.csv')\n",
    "data_X.drop('calc_id', 1, inplace = True)\n",
    "\n",
    "data_X = data_X.iloc[:-1576]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94393558-b16e-4a10-b837-723911c3546f",
   "metadata": {},
   "source": [
    "## Заявление задачи для SALib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca880b6-cf5a-4e77-adbb-b4c16082077e",
   "metadata": {},
   "source": [
    "В SALib анализ чувствительности Соболя происходит по следующему алгоритму: сначала задается словарь, описывающий задачу - список параметров, и границы этих параметров. Затем этот словарь используется в анализе. Сначала генерируется выборка входных параметров для модели с помощью `SALib.satelli.sample()`, а затем эта выборка должна передаться в модель. Данные из этой модели передаются в `SALib.sobol.analyze()`, который уже вычисляет индексы Соболя на основе сгенерированной выборки входных параметров и выходных данных модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e7ed4657-157a-4cd4-b8e6-c357cb093680",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = {\n",
    "    'num_vars': 28,\n",
    "    'names': list(data_X.columns),\n",
    "    'bounds': [[data_X['boundary_condition_NZ'].min(), data_X['boundary_condition_NZ'].max()],\n",
    "               [data_X['boundary_condition_X'].min(), data_X['boundary_condition_X'].max()],\n",
    "               [data_X['boundary_condition_Y'].min(), data_X['boundary_condition_Y'].max()],\n",
    "               [data_X['boundary_condition_NX'].min(), data_X['boundary_condition_NX'].max()],\n",
    "               [data_X['boundary_condition_NY'].min(), data_X['boundary_condition_NY'].max()],\n",
    "               [data_X['boundary_condition_Z'].min(), data_X['boundary_condition_Z'].max()],\n",
    "               [data_X['load_time'].min(), data_X['load_time'].max()],\n",
    "               [data_X['rock_young_constant'].min(), data_X['rock_young_constant'].max()],\n",
    "               [data_X['rock_alpha_constant'].min(), data_X['rock_alpha_constant'].max()],\n",
    "               [data_X['concrete_init_temp'].min(), data_X['concrete_init_temp'].max()],\n",
    "               [data_X['concrete_cheat'].min(), data_X['concrete_cheat'].max()],\n",
    "               [data_X['concrete_dt'].min(), data_X['concrete_dt'].max()],\n",
    "               [data_X['concrete_norm_coeff'].min(), data_X['concrete_norm_coeff'].max()],\n",
    "               [data_X['concrete_young_constant'].min(), data_X['concrete_young_constant'].max()],\n",
    "               [data_X['concrete_alpha_constant'].min(), data_X['concrete_alpha_constant'].max()],\n",
    "               [data_X['concrete_strength_time'].min(), data_X['concrete_strength_time'].max()],\n",
    "               [data_X['steel_init_temp'].min(), data_X['steel_init_temp'].max()],\n",
    "               [data_X['bentonite_init_temp'].min(), data_X['bentonite_init_temp'].max()],\n",
    "               [data_X['bentonite_cheat'].min(), data_X['bentonite_cheat'].max()],\n",
    "               [data_X['bentonite_dt'].min(), data_X['bentonite_dt'].max()],\n",
    "               [data_X['bentonite_young_constant'].min(), data_X['bentonite_young_constant'].max()],\n",
    "               [data_X['bentonite_alpha_constant'].min(), data_X['bentonite_alpha_constant'].max()],\n",
    "               [data_X['rw_init_temp'].min(), data_X['rw_init_temp'].max()],\n",
    "               [data_X['rw_cheat'].min(), data_X['rw_cheat'].max()],\n",
    "               [data_X['rw_dt'].min(), data_X['rw_dt'].max()],\n",
    "               [data_X['rw_young_constant'].min(), data_X['rw_young_constant'].max()],\n",
    "               [data_X['rw_alpha_constant'].min(), data_X['rw_alpha_constant'].max()],\n",
    "               [data_X['rw_norm_coeff'].min(), data_X['rw_norm_coeff'].max()]]\n",
    "}\n",
    "\n",
    "\n",
    "param_values = saltelli.sample(problem, 128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f41661-b704-49e7-beeb-9ad84404f87b",
   "metadata": {},
   "source": [
    "Так как модель мы не имеем, а имеем только набор входных и выходных данных, нужно что-то придумать. Первым в голову приходит создание своей модели, которая научится на наших данных. Для этого я использовал `RandomForestRegressor`. `Cross_val_score` получился равным около 0.95, среднее квадратичное test данных от train было очень близким к нулю. Также я провел `GridSearchCV`, чтобы определить оптимальные входные параметры модели *(хотя можно вроде бы еще поизучать этот вопрос, потому что не то чтоб много разных параметров я отправил на изучение, ибо проходит поиск небыстро)*, получилось, что `n_estimators` выгоднее выбирать 75. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "08e68479-cf74-499c-93c7-d5d0d7dd4424",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = ensemble.RandomForestRegressor(n_estimators = 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebc3d29-a3de-46a3-aa3c-8df857d8b448",
   "metadata": {},
   "source": [
    "Здесь и происходит весь анализ. Поясню за некоторые моменты: `.iloc[:-1576]` здесь нужен, чтобы выкинуть лишние таргет данные, потому что `satelli.sample()` генерирует `N * (2D + 1)` параметров, где N обязательно должно быть степенью двойки (в моем случае 128), а D - количество входных признаков. Думаю, что потеря 1576 данных из 9000 не сильно сказывается на обучении, тем более, что я проверял точность обучения. Далее я обучал модель полностью `sample.csv` и `temp.csv`, а проводил предикт на сгенерированной `satelli.sample()` выборке. Таким образом, мы избавились от необходимости обращаться к физической модели, потому что обучили свою на случайном лесе. Ну и далее мы просто передаем выходные данные в `sobol.analyze()`, и получаем нужные индексы. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54eb989-8538-45bc-8992-8299d0638e71",
   "metadata": {},
   "source": [
    "Время работы следующей клетки ~10 минут, как и последующей за ней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d0fb4360-3324-4f89-8a27-a0b28ace402c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "for k in range(1,14,1):\n",
    "    observe_point = k\n",
    "    points.remove(observe_point)\n",
    "    \n",
    "    data_temp = pd.read_csv('temp.csv')\n",
    "    data_temp.drop('calc_id', 1, inplace = True)\n",
    "    \n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "        for i in points:\n",
    "            data_temp.drop(columns = 'temp_point'+str(i)+'_'+str(j)+'days', inplace = True)\n",
    "    data_temp = data_temp.iloc[:-1576]\n",
    "\n",
    "\n",
    "\n",
    "    sobol_indices = pd.DataFrame(index = list(data_X.columns))\n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "\n",
    "        rf_clf.fit(data_X, data_temp['temp_point' + str(observe_point)+ '_' + str(j) + 'days'])\n",
    "        predicted = rf_clf.predict(param_values)\n",
    "        Si = sobol.analyze(problem, predicted)\n",
    "        sobol_indices.insert(len(sobol_indices.columns), 'ST_'+str(observe_point)+'_'+str(j)+'days' ,Si['ST'])\n",
    "\n",
    "    sobol_indices.to_csv('sobol_indices_temp_point' +str(k)+'.csv', float_format='%.6f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5ed45d-4ed6-47b7-a2e1-6833488bf829",
   "metadata": {},
   "source": [
    "То же самое для `stress.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b0dbe522-c376-4139-ad8c-4398e4d5225b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "points = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "for k in range(1,14,1):\n",
    "    observe_point = k\n",
    "    points.remove(observe_point)\n",
    "    \n",
    "    data_stress = pd.read_csv('stress.csv')\n",
    "    data_stress.drop('calc_id', 1, inplace = True)\n",
    "    \n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "        for i in points:\n",
    "            data_stress.drop(columns = 'stress_point'+str(i)+'_'+str(j)+'days', inplace = True)\n",
    "    data_stress = data_stress.iloc[:-1576]\n",
    "\n",
    "\n",
    "\n",
    "    sobol_indices = pd.DataFrame(index = list(data_X.columns))\n",
    "    for j in [5,10,20,30,40,50,150,1000]:\n",
    "\n",
    "        rf_clf.fit(data_X, data_stress['stress_point' + str(observe_point)+ '_' + str(j) + 'days'])\n",
    "        predicted = rf_clf.predict(param_values)\n",
    "        Si = sobol.analyze(problem, predicted)\n",
    "        sobol_indices.insert(len(sobol_indices.columns), 'ST_'+str(observe_point)+'_'+str(j)+'days' ,Si['ST'])\n",
    "\n",
    "    sobol_indices.to_csv('sobol_indices_stress_point' +str(k)+'.csv', float_format='%.7f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddda91b-0b92-4f14-b064-2eee30b2e2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
